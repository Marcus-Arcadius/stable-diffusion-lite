{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stable Diffusion Lite",
      "provenance": [],
      "background_execution": "on",
      "machine_shape": "hm",
      "collapsed_sections": [
        "0GOr30k9Gh1L",
        "o8hwXeJhIfJl",
        "QEX4emjII54P"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vincefav/stable-diffusion-lite/blob/main/Stable_Diffusion_Lite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stable Diffusion Lite by [FutureArt](https://twitter.com/future__art) and [Pharmapsychotic](https://twitter.com/pharmapsychotic)\n",
        "\n",
        "v 1.1.1, last updated 8/23/22\n",
        "\n",
        "Generate images with CompVis/Stability [Stable Diffusion](https://github.com/CompVis/stable-diffusion) with bonus [KLMS sampling](https://github.com/crowsonkb/k-diffusion.git) from [@RiversHaveWings](https://twitter.com/RiversHaveWings).\n",
        "\n",
        "This notebook is a pretty faithful copy of [pharmapsychotic's Stable Diffusion notebook](https://colab.research.google.com/github/pharmapsychotic/ai-notebooks/blob/main/pharmapsychotic_Stable_Diffusion.ipynb). I just simplified some settings, wrote some instructions, and added the ability to queue up multiple prompts. If you want more control over your settings, such as selecting what sampler to use, then I suggest using the original notebook.\n",
        "\n",
        "I plan to make improvements to this notebook. Please [follow me on Twitter](https://twitter.com/future__art) or [watch my github](https://github.com/vincefav/stable-diffusion-lite) to get updates. (I'm a UX designer btw, so get in touch if you like this notebook and want to hire me!)\n",
        "\n",
        "### How to download the model\n",
        "\n",
        "- Visit https://huggingface.co/CompVis/stable-diffusion-v-1-4-original and agree to the terms and conditions.\n",
        "- Click the **Files and versions** tab\n",
        "- Click **stable-diffusion-v-1-4-original**\n",
        "- Click the **download** link where it says *This file is stored with Git LFS . It is too big to display, but you can still download it.*\n",
        "- If you have [Google Drive for desktop](https://www.google.com/drive/download/) (highly recommended), you can save it directly to your **AI/models** directory.\n",
        "  - Otherwise, download it and re-upload it to your [Google Drive](https://drive.google.com) in the **AI/models** directory. (This is risky, as the upload may time out.)"
      ],
      "metadata": {
        "id": "UU52ZvES6-1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check GPU\n",
        "!nvidia-smi -L"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uc5OwvKdjRJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup"
      ],
      "metadata": {
        "id": "0GOr30k9Gh1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Mount Google Drive and Prepare Folders\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "outputs_path = \"/content/gdrive/MyDrive/AI/Stable Diffusion/images_out\"\n",
        "!mkdir -p $outputs_path\n",
        "print(f\"Outputs will be saved to {outputs_path}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DeqQ7pt1zdI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Installation\n",
        "!pip install pytorch-lightning torch-fidelity\n",
        "!pip install numpy omegaconf einops kornia pytorch-lightning\n",
        "!pip install albumentations transformers\n",
        "!pip install ftfy jsonmerge resize-right torchdiffeq tqdm\n",
        "\n",
        "!git clone https://github.com/CompVis/stable-diffusion\n",
        "%cd stable-diffusion/\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "!git clone https://github.com/openai/CLIP.git\n",
        "!git clone https://github.com/crowsonkb/k-diffusion.git\n",
        "\n",
        "import sys\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"./CLIP\")\n",
        "sys.path.append('./taming-transformers')\n",
        "sys.path.append('./k-diffusion')\n",
        "\n",
        "!echo '' > ./k-diffusion/k_diffusion/__init__.py\n"
      ],
      "metadata": {
        "id": "XA1NNcxM724U",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown You need to get the model weights yourself and put on Google Drive or this Colab instance\n",
        "checkpoint_model_file = \"/content/gdrive/MyDrive/AI/models/sd-v1-4.ckpt\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "mTjVEfsNlDly",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define helper functions\n",
        "\n",
        "import argparse, gc, json, os, random, sys, time, glob, requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import PIL\n",
        "from contextlib import contextmanager, nullcontext\n",
        "from einops import rearrange, repeat\n",
        "from IPython.display import display, clear_output\n",
        "from itertools import islice\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from pytorch_lightning import seed_everything\n",
        "from torch.cuda.amp import autocast\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler\n",
        "\n",
        "from k_diffusion.sampling import sample_lms\n",
        "from k_diffusion.external import CompVisDenoiser\n",
        "\n",
        "\n",
        "def chunk(it, size):\n",
        "    it = iter(it)\n",
        "    return iter(lambda: tuple(islice(it, size)), ())\n",
        "\n",
        "def load_model_from_config(config, ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "class CFGDenoiser(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.inner_model = model\n",
        "\n",
        "    def forward(self, x, sigma, uncond, cond, cond_scale):\n",
        "        x_in = torch.cat([x] * 2)\n",
        "        sigma_in = torch.cat([sigma] * 2)\n",
        "        cond_in = torch.cat([uncond, cond])\n",
        "        uncond, cond = self.inner_model(x_in, sigma_in, cond=cond_in).chunk(2)\n",
        "        return uncond + (cond - uncond) * cond_scale\n",
        "\n",
        "class config():\n",
        "    def __init__(self):\n",
        "        self.ckpt = checkpoint_model_file\n",
        "        self.config = 'configs/stable-diffusion/v1-inference.yaml'\n",
        "        self.ddim_eta = 0.0\n",
        "        self.ddim_steps = 100\n",
        "        self.fixed_code = True\n",
        "        self.init_img = None\n",
        "        self.n_iter = 1\n",
        "        self.n_samples = 1\n",
        "        self.outdir = \"\"\n",
        "        self.precision = 'full' # 'autocast'\n",
        "        self.prompt = \"\"\n",
        "        self.sampler = 'klms'\n",
        "        self.scale = 7.5\n",
        "        self.seed = 42\n",
        "        self.strength = 0.75 # strength for noising/unnoising. 1.0 corresponds to full destruction of information in init image\n",
        "        self.H = 512\n",
        "        self.W = 512\n",
        "        self.C = 4\n",
        "        self.f = 8\n",
        "      \n",
        "def load_img(path, w, h):\n",
        "    if path.startswith('http://') or path.startswith('https://'):\n",
        "        image = Image.open(requests.get(path, stream=True).raw).convert('RGB')\n",
        "    else:\n",
        "        if os.path.isdir(path):\n",
        "            files = [file for file in os.listdir(path) if file.endswith('.png') or file .endswith('.jpg')]\n",
        "            path = os.path.join(path, random.choice(files))\n",
        "            print(f\"Chose random init image {path}\")\n",
        "        image = Image.open(path).convert('RGB')\n",
        "    image = image.resize((w, h), Image.LANCZOS)\n",
        "    w, h = image.size\n",
        "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
        "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float32) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.*image - 1.\n",
        "\n",
        "opt = config()\n",
        "config = OmegaConf.load(f\"{opt.config}\")\n",
        "model = load_model_from_config(config, f\"{opt.ckpt}\")\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = model.to(device)\n",
        "batch_idx = 0\n",
        "sample_idx = 0\n",
        "\n",
        "def generate(opt):\n",
        "    global sample_idx\n",
        "    seed_everything(opt.seed)\n",
        "    os.makedirs(opt.outdir, exist_ok=True)\n",
        "\n",
        "    if opt.sampler == 'plms':\n",
        "        sampler = PLMSSampler(model)\n",
        "    else:\n",
        "        sampler = DDIMSampler(model)\n",
        "\n",
        "    model_wrap = CompVisDenoiser(model)       \n",
        "    batch_size = opt.n_samples\n",
        "    prompt = opt.prompt\n",
        "    assert prompt is not None\n",
        "    data = [batch_size * [prompt]]\n",
        "    init_latent = None\n",
        "\n",
        "    if opt.init_img != None and opt.init_img != '':\n",
        "        init_image = load_img(opt.init_img, opt.W, opt.H).to(device)\n",
        "        init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
        "        init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
        "\n",
        "    sampler.make_schedule(ddim_num_steps=opt.ddim_steps, ddim_eta=opt.ddim_eta, verbose=False)\n",
        "\n",
        "    t_enc = int(opt.strength * opt.ddim_steps)\n",
        "\n",
        "    start_code = None\n",
        "    if opt.fixed_code and init_latent == None:\n",
        "        start_code = torch.randn([opt.n_samples, opt.C, opt.H // opt.f, opt.W // opt.f], device=device)\n",
        "\n",
        "    precision_scope = autocast if opt.precision == \"autocast\" else nullcontext\n",
        "\n",
        "    images = []\n",
        "    with torch.no_grad():\n",
        "        with precision_scope(\"cuda\"):\n",
        "            with model.ema_scope():\n",
        "                for n in range(opt.n_iter):\n",
        "                    for prompts in data:\n",
        "                        uc = None\n",
        "                        if opt.scale != 1.0:\n",
        "                            uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
        "                        if isinstance(prompts, tuple):\n",
        "                            prompts = list(prompts)\n",
        "                        c = model.get_learned_conditioning(prompts)\n",
        "\n",
        "                        if init_latent != None:\n",
        "                            z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
        "                            samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=opt.scale,\n",
        "                                                    unconditional_conditioning=uc,)\n",
        "                        else:\n",
        "\n",
        "                            if opt.sampler == 'klms':\n",
        "                                print(\"Using KLMS sampling\")\n",
        "                                shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
        "                                sigmas = model_wrap.get_sigmas(opt.ddim_steps)\n",
        "                                model_wrap_cfg = CFGDenoiser(model_wrap)\n",
        "                                x = torch.randn([opt.n_samples, *shape], device=device) * sigmas[0]\n",
        "                                extra_args = {'cond': c, 'uncond': uc, 'cond_scale': opt.scale}\n",
        "                                samples = sample_lms(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=False)\n",
        "                            else:\n",
        "                                shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
        "                                samples, _ = sampler.sample(S=opt.ddim_steps,\n",
        "                                                                conditioning=c,\n",
        "                                                                batch_size=opt.n_samples,\n",
        "                                                                shape=shape,\n",
        "                                                                verbose=False,\n",
        "                                                                unconditional_guidance_scale=opt.scale,\n",
        "                                                                unconditional_conditioning=uc,\n",
        "                                                                eta=opt.ddim_eta,\n",
        "                                                                x_T=start_code)\n",
        "\n",
        "                        x_samples = model.decode_first_stage(samples)\n",
        "                        x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "\n",
        "                        for x_sample in x_samples:\n",
        "                            x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                            images.append(Image.fromarray(x_sample.astype(np.uint8)))\n",
        "                            filepath = os.path.join(opt.outdir, f\"{batch_name}({batch_idx}) {opt.seed}.png\")#####\n",
        "                            print(f\"Saving to {filepath}\")\n",
        "                            Image.fromarray(x_sample.astype(np.uint8)).save(filepath)\n",
        "                            sample_idx += 1\n",
        "    return images\n"
      ],
      "metadata": {
        "id": "bcHsbr3hblrk",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Diffuse!"
      ],
      "metadata": {
        "id": "Bs0rYSMkGqYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Image creation\n",
        "\n",
        "#@markdown Files are saved as the batch name, followed by the batch number, followed by the image seed.\n",
        "\n",
        "#@markdown `batch_name`: name for subfolder and filenames<br>\n",
        "#@markdown `width` and `height`: image dimensions<br>\n",
        "#@markdown `guidance_scale`: strength of text prompt<br>\n",
        "#@markdown `steps`: number of diffusion steps<br>\n",
        "#@markdown `number_of_images`: how many images you want to generate in this batch<br>\n",
        "#@markdown `init_image_or_folder`: url or path to an image, or path to a folder to pick random images from<br>\n",
        "#@markdown `init_strength`: from 0.0 to 1.0 how much the init image is used<br>\n",
        "\n",
        "#@markdown \n",
        "\n",
        "batch_name = \"cat\" #@param {type:\"string\"}\n",
        "prompt = \"a cute portrait of a cat, digital art by James Gurney, trending on artstation\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown\n",
        "\n",
        "# I hid some settings: sampler, ddim_eta, and seed. Add the @ after param to expose them again.\n",
        "width = 512 #@param {type:\"integer\"}\n",
        "height = 512 #@param {type:\"integer\"}\n",
        "width_height = [width, height] # param{type: 'raw'}\n",
        "guidance_scale = 8 #@param {type:\"slider\", min:0, max:20, step:0.5}\n",
        "steps = 50 #@param {type:\"integer\"}\n",
        "samples_per_batch = 1 # not exposed, you can do 2 or more based on GPU ram, if get CUDA out of memory need to restart runtime\n",
        "number_of_images = 50 #@param {type:\"integer\"}\n",
        "sampler = 'klms' # param [\"klms\",\"plms\", \"ddim\"]\n",
        "ddim_eta = 0.75 # param {type:\"number\"}\n",
        "seed = -1 # param {type:\"integer\"}\n",
        "save_settings_file = False#@param{type:\"boolean\"}\n",
        "\n",
        "#@markdown \n",
        "\n",
        "init_image_or_folder = \"\" #@param {type:\"string\"}\n",
        "init_strength = 0.4 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "\n",
        "#@markdown \n",
        "\n",
        "\n",
        "opt.init_img = init_image_or_folder\n",
        "opt.ddim_steps = steps\n",
        "opt.n_iter = 1\n",
        "opt.n_samples = samples_per_batch\n",
        "opt.outdir = os.path.join(outputs_path, batch_name)\n",
        "opt.prompt = prompt\n",
        "opt.sampler = sampler\n",
        "opt.scale = guidance_scale\n",
        "opt.seed = random.randint(0, 2**32) if seed == -1 else seed\n",
        "opt.strength = max(0.0, min(1.0, 1.0 - init_strength))\n",
        "opt.W = width_height[0]\n",
        "opt.H = width_height[1]\n",
        "\n",
        "if opt.strength >= 1 or init_image_or_folder == None:\n",
        "    opt.init_img = \"\"\n",
        "\n",
        "if opt.init_img != None and opt.init_img != '':\n",
        "    opt.sampler = 'ddim'\n",
        "\n",
        "if opt.sampler != 'ddim':\n",
        "    opt.ddim_eta = 0.0\n",
        "\n",
        "# save settings\n",
        "settings = {\n",
        "    'ddim_eta': ddim_eta,\n",
        "    'guidance_scale': guidance_scale,\n",
        "    'init_image': init_image_or_folder,\n",
        "    'init_strength': init_strength,\n",
        "    'number_of_images': number_of_images,\n",
        "    'prompt': prompt,\n",
        "    'sampler': sampler,\n",
        "    'samples_per_batch': samples_per_batch,\n",
        "    'seed': opt.seed,\n",
        "    'steps': steps,\n",
        "    'width': opt.W,\n",
        "    'height': opt.H,\n",
        "}\n",
        "os.makedirs(opt.outdir, exist_ok=True)\n",
        "while os.path.isfile(f\"{opt.outdir}/{batch_name}({batch_idx})_settings.txt\"):\n",
        "    batch_idx += 1\n",
        "if save_settings_file:\n",
        "  with open(f\"{opt.outdir}/{batch_name}({batch_idx})_settings.txt\", \"w+\", encoding=\"utf-8\") as f:\n",
        "      json.dump(settings, f, ensure_ascii=False, indent=4)\n",
        "with open(f\"{opt.outdir}/prompt.txt\", 'w') as f:\n",
        "        f.write(prompt)\n",
        "sample_idx = 0\n",
        "\n",
        "for i in range(number_of_images):\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    images = generate(opt)\n",
        "\n",
        "    clear_output(wait=True)\n",
        "    print(f\"Used seed: {opt.seed}\")\n",
        "    print(f\"Saved to: {opt.outdir}\")\n",
        "    for image in images:\n",
        "        display(image)\n",
        "\n",
        "    opt.seed += 1\n"
      ],
      "metadata": {
        "id": "wzmVAdZ1-5tE",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Prompt Queuing\n",
        "\n",
        "Use this section if you want to run multiple prompts and create several images."
      ],
      "metadata": {
        "id": "o8hwXeJhIfJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put each of your prompts on a new line\n",
        "\n",
        "prompts = '''\n",
        "a beautiful painting of a waterlily pond\n",
        "a beautiful painting of a serene landscape\n",
        "a beautiful still-life painting of flowers in a vase\n",
        "'''"
      ],
      "metadata": {
        "id": "jvU-T058oLsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Image creation\n",
        "\n",
        "#@markdown `batch_name`: name for subfolder and filenames<br>\n",
        "#@markdown `width` and `height`: image dimensions<br>\n",
        "#@markdown `guidance_scale`: strength of text prompt<br>\n",
        "#@markdown `steps`: number of diffusion steps<br>\n",
        "#@markdown `number_of_images`: how many images you want to generate in this batch<br>\n",
        "\n",
        "#@markdown\n",
        "\n",
        "import re\n",
        "\n",
        "if type(prompts) is str:\n",
        "  prompts = [i for i in prompts.split('\\n') if i]\n",
        "\n",
        "def clean_string(s):\n",
        "    s = ''.join([c for c in s if (re.match('[a-zA-Z0-9 _]', c) or ord(c) > 127)]).strip()\n",
        "    if len(s) > 200:\n",
        "        return (s[:150]).strip()\n",
        "    return s\n",
        "\n",
        "width = 512 #@param {type:\"integer\"}\n",
        "height = 512 #@param {type:\"integer\"}\n",
        "width_height = [width, height] # param{type: 'raw'}\n",
        "guidance_scale = 7.5 #@param {type:\"slider\", min:0, max:20, step:0.5}\n",
        "steps = 50 #@param {type:\"integer\"}\n",
        "samples_per_batch = 1 # not exposed, you can do 2 or more based on GPU ram, if get CUDA out of memory need to restart runtime\n",
        "number_of_images = 10 #@param {type:\"integer\"}\n",
        "sampler = 'klms' # param [\"klms\",\"plms\", \"ddim\"]\n",
        "ddim_eta = 0.75 # param {type:\"number\"}\n",
        "seed = -1 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown\n",
        "\n",
        "#@markdown Init image\n",
        "init_image_or_folder = \"\" #@param {type:\"string\"}\n",
        "init_strength = 0.4 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "\n",
        "save_settings_file = False#@param{type:\"boolean\"}\n",
        "\n",
        "for prompt in prompts:\n",
        "  batch_name = clean_string(prompt)\n",
        "  opt.outdir = os.path.join(outputs_path, batch_name) \n",
        "  opt.init_img = init_image_or_folder\n",
        "  opt.ddim_steps = steps\n",
        "  opt.n_iter = 1\n",
        "  opt.n_samples = samples_per_batch\n",
        "  opt.prompt = prompt\n",
        "  opt.sampler = sampler\n",
        "  opt.scale = guidance_scale\n",
        "  opt.seed = random.randint(0, 2**32) if seed == -1 else seed\n",
        "  opt.strength = max(0.0, min(1.0, 1.0 - init_strength))\n",
        "  opt.W = width_height[0]\n",
        "  opt.H = width_height[1]\n",
        "\n",
        "  if opt.strength >= 1 or init_image_or_folder == None:\n",
        "      opt.init_img = \"\"\n",
        "\n",
        "  if opt.init_img != None and opt.init_img != '':\n",
        "      opt.sampler = 'ddim'\n",
        "\n",
        "  if opt.sampler != 'ddim':\n",
        "      opt.ddim_eta = 0.0\n",
        "\n",
        "  # save settings\n",
        "  settings = {\n",
        "      'ddim_eta': ddim_eta,\n",
        "      'guidance_scale': guidance_scale,\n",
        "      'init_image': init_image_or_folder,\n",
        "      'init_strength': init_strength,\n",
        "      'number_of_images': number_of_images,\n",
        "      'prompt': prompt,\n",
        "      'sampler': sampler,\n",
        "      'samples_per_batch': samples_per_batch,\n",
        "      'seed': opt.seed,\n",
        "      'steps': steps,\n",
        "      'width': opt.W,\n",
        "      'height': opt.H,\n",
        "  }\n",
        "  os.makedirs(opt.outdir, exist_ok=True)\n",
        "  while os.path.isfile(f\"{opt.outdir}/{batch_name}({batch_idx})_settings.txt\"):\n",
        "      batch_idx += 1\n",
        "  if save_settings_file:\n",
        "    with open(f\"{opt.outdir}/{batch_name}({batch_idx})_settings.txt\", \"w+\", encoding=\"utf-8\") as f:\n",
        "        json.dump(settings, f, ensure_ascii=False, indent=4)\n",
        "  with open(f\"{opt.outdir}/prompt.txt\", 'w') as f:\n",
        "        f.write(prompt)\n",
        "  sample_idx = 0\n",
        "\n",
        "  for i in range(number_of_images):\n",
        "      gc.collect()\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "      images = generate(opt)\n",
        "\n",
        "      clear_output(wait=True)\n",
        "      print(f\"Used seed: {opt.seed}\")\n",
        "      print(f\"Saved to: {opt.outdir}\")\n",
        "      for image in images:\n",
        "          display(image)\n",
        "\n",
        "      opt.seed += 1\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iPfN1gYgoJkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Seed Mining\n",
        "\n",
        "Use this section if you want to keep the same seed as you run it through different prompt variations. (The seed won't change in each successive run.)"
      ],
      "metadata": {
        "id": "QEX4emjII54P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put each of your prompts on a new line. They should be similar with very slight tweaks.\n",
        "\n",
        "prompts = '''\n",
        "a beautiful still-life painting of flowers in a vase by claude monet\n",
        "a beautiful still-life painting of flowers in a vase by vincent van gogh\n",
        "a beautiful still-life painting of flowers in a vase by john singer sargent\n",
        "'''"
      ],
      "metadata": {
        "id": "BMkwa9jMI54R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Image creation\n",
        "#@markdown `batch_name`: the folder to which your images will save<br>\n",
        "#@markdown `width` and `height`: image dimensions<br>\n",
        "#@markdown `guidance_scale`: strength of text prompt<br>\n",
        "#@markdown `steps`: number of diffusion steps<br>\n",
        "#@markdown `number_of_images`: you probably only need 1 for seed mining<br>\n",
        "#@markdown `save_settings_file`: I recommend keeping this on, so you can match your images to your prompts<br>\n",
        "\n",
        "\n",
        "#@markdown\n",
        "\n",
        "import re\n",
        "\n",
        "if type(prompts) is str:\n",
        "  prompts = [i for i in prompts.split('\\n') if i]\n",
        "\n",
        "def clean_string(s):\n",
        "    s = ''.join([c for c in s if (re.match('[a-zA-Z0-9 _]', c) or ord(c) > 127)]).strip()\n",
        "    if len(s) > 200:\n",
        "        return (s[:150]).strip()\n",
        "    return s\n",
        "\n",
        "batch_name = 'flowers in a vase'#@param{type:\"string\"}\n",
        "opt.outdir = os.path.join(outputs_path, batch_name) \n",
        "width = 512 #@param {type:\"integer\"}\n",
        "height = 512 #@param {type:\"integer\"}\n",
        "width_height = [width, height] # param{type: 'raw'}\n",
        "guidance_scale = 7.5 #@param {type:\"slider\", min:0, max:20, step:0.5}\n",
        "steps = 50 #@param {type:\"integer\"}\n",
        "samples_per_batch = 1 # not exposed, you can do 2 or more based on GPU ram, if get CUDA out of memory need to restart runtime\n",
        "number_of_images = 1 #@param {type:\"integer\"}\n",
        "sampler = 'klms' # param [\"klms\",\"plms\", \"ddim\"]\n",
        "ddim_eta = 0.75 # param {type:\"number\"}\n",
        "seed = 123456789 #@param {type:\"integer\"}\n",
        "if seed == -1:\n",
        "  raise ValueError(\"Seed mining won't work properly with a random seed.\")\n",
        "\n",
        "#@markdown\n",
        "\n",
        "#@markdown Init image\n",
        "init_image_or_folder = \"\" #@param {type:\"string\"}\n",
        "init_strength = 0.4 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "save_settings_file = True#@param{type:\"boolean\"}\n",
        "\n",
        "for prompt in prompts:  \n",
        "  opt.init_img = init_image_or_folder\n",
        "  opt.ddim_steps = steps\n",
        "  opt.n_iter = 1\n",
        "  opt.n_samples = samples_per_batch\n",
        "  opt.prompt = prompt\n",
        "  opt.sampler = sampler\n",
        "  opt.scale = guidance_scale\n",
        "  opt.seed = random.randint(0, 2**32) if seed == -1 else seed\n",
        "  opt.strength = max(0.0, min(1.0, 1.0 - init_strength))\n",
        "  opt.W = width_height[0]\n",
        "  opt.H = width_height[1]\n",
        "\n",
        "  if opt.strength >= 1 or init_image_or_folder == None:\n",
        "      opt.init_img = \"\"\n",
        "\n",
        "  if opt.init_img != None and opt.init_img != '':\n",
        "      opt.sampler = 'ddim'\n",
        "\n",
        "  if opt.sampler != 'ddim':\n",
        "      opt.ddim_eta = 0.0\n",
        "\n",
        "  # save settings\n",
        "  settings = {\n",
        "      'ddim_eta': ddim_eta,\n",
        "      'guidance_scale': guidance_scale,\n",
        "      'init_image': init_image_or_folder,\n",
        "      'init_strength': init_strength,\n",
        "      'number_of_images': number_of_images,\n",
        "      'prompt': prompt,\n",
        "      'sampler': sampler,\n",
        "      'samples_per_batch': samples_per_batch,\n",
        "      'seed': opt.seed,\n",
        "      'steps': steps,\n",
        "      'width': opt.W,\n",
        "      'height': opt.H,\n",
        "  }\n",
        "  os.makedirs(opt.outdir, exist_ok=True)\n",
        "  while os.path.isfile(f\"{opt.outdir}/{batch_name}({batch_idx})_settings.txt\"):\n",
        "      batch_idx += 1\n",
        "\n",
        "  with open(f\"{opt.outdir}/prompts.txt\", 'a') as f:\n",
        "      f.write(prompt + '\\n\\n')\n",
        "  if save_settings_file:\n",
        "    with open(f\"{opt.outdir}/{batch_name}({batch_idx})_settings.txt\", \"w+\", encoding=\"utf-8\") as f:\n",
        "        json.dump(settings, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "  sample_idx = 0\n",
        "\n",
        "  for i in range(number_of_images):\n",
        "      gc.collect()\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "      images = generate(opt)\n",
        "\n",
        "      clear_output(wait=True)\n",
        "      print(f\"Used seed: {opt.seed}\")\n",
        "      print(f\"Saved to: {opt.outdir}\")\n",
        "      for image in images:\n",
        "          display(image)\n",
        "\n",
        "      # opt.seed += 1\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MGdVR60ZI54T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}